{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de139cbf",
   "metadata": {},
   "source": [
    "# 04 â€” Hyperparameter Optimization with Ray Tune\n",
    "*Generated: 2025-10-14 18:31*\n",
    "\n",
    "This notebook shows two ways to do HPO with **Ray**:\n",
    "1) **Native Ray Tune**: define a trainable function; works with any model (recommended).\n",
    "2) **tune-sklearn**: sklearn-style `TuneSearchCV` if available.\n",
    "\n",
    "It integrates with the **FlexibleModel** wrapper from file `03_model_controller.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# !pip -q install 'ray[tune]>=2.9.0' tune-sklearn imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58140c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "try:\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.tune.schedulers import ASHAScheduler\n",
    "    from ray.tune.search.hyperopt import HyperOptSearch\n",
    "    HAS_RAY = True\n",
    "except Exception as e:\n",
    "    HAS_RAY = False\n",
    "    warnings.warn(f'Ray not available: {e}')\n",
    "\n",
    "try:\n",
    "    from tune_sklearn import TuneSearchCV\n",
    "    HAS_TUNE_SKLEARN = True\n",
    "except Exception:\n",
    "    HAS_TUNE_SKLEARN = False\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception:\n",
    "    SMOTE = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If you placed the class in the same dir as a .py, you could do:\n",
    "    # from model_controller import FlexibleModel\n",
    "    FlexibleModel  # noqa: just to check name exists if you ran 03 notebook already\n",
    "except NameError:\n",
    "    # Minimal fallback wrapper (subset of 03) to keep this notebook runnable standalone\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.cluster import KMeans\n",
    "    try:\n",
    "        from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    except Exception:\n",
    "        ImbPipeline = None\n",
    "\n",
    "    def _ensure_2d(X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            return X.to_frame()\n",
    "        return X\n",
    "\n",
    "    class FlexibleModel:\n",
    "        def __init__(self, scaler=None, sampler=None, clf=None):\n",
    "            self.scaler = scaler\n",
    "            self.sampler = sampler\n",
    "            self.clf = clf or LogisticRegression(max_iter=1000)\n",
    "            self.pipeline = None\n",
    "        def _pre(self, X):\n",
    "            if self.scaler is None:\n",
    "                return None\n",
    "            cols = X.select_dtypes(include=[np.number]).columns.tolist() if isinstance(X, pd.DataFrame) else list(range(X.shape[1]))\n",
    "            num = Pipeline([\n",
    "                ('imp', SimpleImputer(strategy='median')),\n",
    "                ('sc', self.scaler)\n",
    "            ])\n",
    "            return ColumnTransformer([('num', num, cols)], remainder='passthrough')\n",
    "        def _make(self, X):\n",
    "            pre = self._pre(X)\n",
    "            steps = [('pre', pre)] if pre is not None else []\n",
    "            if self.sampler is not None:\n",
    "                if ImbPipeline is None:\n",
    "                    raise RuntimeError('imblearn missing for sampler usage')\n",
    "                self.pipeline = ImbPipeline(steps + [('sampler', self.sampler), ('clf', self.clf)])\n",
    "            else:\n",
    "                from sklearn.pipeline import Pipeline\n",
    "                self.pipeline = Pipeline(steps + [('clf', self.clf)])\n",
    "        def fit(self, X, y):\n",
    "            X = _ensure_2d(X)\n",
    "            self._make(X)\n",
    "            self.pipeline.fit(X, y)\n",
    "            return self\n",
    "        def predict(self, X, y=None):\n",
    "            X = _ensure_2d(X)\n",
    "            return self.pipeline.predict(X)\n",
    "        def predict_proba(self, X):\n",
    "            X = _ensure_2d(X)\n",
    "            if hasattr(self.pipeline.named_steps['clf'], 'predict_proba'):\n",
    "                return self.pipeline.predict_proba(X)[:,1]\n",
    "            if hasattr(self.pipeline.named_steps['clf'], 'decision_function'):\n",
    "                s = self.pipeline.decision_function(X)\n",
    "                s = (s - s.min())/(s.max()-s.min()+1e-12)\n",
    "                return s\n",
    "            return self.pipeline.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06b515",
   "metadata": {},
   "source": [
    "## A) Native Ray Tune (recommended)\n",
    "Define a trainable that builds a `FlexibleModel` from a **config** dict, runs Stratified K-Fold, and reports the **ROC-AUC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0420031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_from_config(config: Dict[str, Any]):\n",
    "    # Build components\n",
    "    scaler = None\n",
    "    if config.get('scaler') == 'standard':\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "    elif config.get('scaler') == 'none':\n",
    "        scaler = None\n",
    "\n",
    "    sampler = None\n",
    "    if config.get('sampler') == 'smote' and SMOTE is not None:\n",
    "        sampler = SMOTE(random_state=RANDOM_STATE, k_neighbors=int(config.get('smote_k', 5)))\n",
    "\n",
    "    clf_type = config.get('clf', 'rf')\n",
    "    if clf_type == 'rf':\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=int(config.get('n_estimators', 200)),\n",
    "            max_depth=None if config.get('max_depth') is None else int(config.get('max_depth')),\n",
    "            min_samples_leaf=int(config.get('min_samples_leaf', 1)),\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    elif clf_type == 'svm':\n",
    "        clf = SVC(\n",
    "            C=float(config.get('C', 1.0)),\n",
    "            kernel=config.get('kernel', 'rbf'),\n",
    "            gamma=config.get('gamma', 'scale'),\n",
    "            probability=True,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported clf: {clf_type}')\n",
    "\n",
    "    return FlexibleModel(scaler=scaler, sampler=sampler, clf=clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e685ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_objective(config: Dict[str, Any], X=None, y=None, n_splits=3):\n",
    "    # Expect X, y passed via tune.with_parameters\n",
    "    model = make_model_from_config(config)\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    for tr, te in cv.split(X, y):\n",
    "        Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "        ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "        model.fit(Xtr, ytr)\n",
    "        yprob = model.predict_proba(Xte)\n",
    "        # safety: flatten prob array\n",
    "        yprob = np.asarray(yprob).reshape(-1)\n",
    "        scores.append(roc_auc_score(yte, yprob))\n",
    "    tune.report(roc_auc=float(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a7fa8",
   "metadata": {},
   "source": [
    "### Example search spaces\n",
    "Choose between RandomForest and SVM configurations with scaling/smote options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eccf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'clf': tune.choice(['rf','svm']),\n",
    "    'scaler': tune.choice(['standard','none']),\n",
    "    'sampler': tune.choice(['smote','none']),\n",
    "    'smote_k': tune.choice([3,5,7])\n",
    "}\n",
    "\n",
    "# RF-only specific params (active when clf='rf')\n",
    "rf_space = {\n",
    "    'n_estimators': tune.qrandint(100, 500, 50),\n",
    "    'max_depth': tune.choice([None, 5, 10, 20]),\n",
    "    'min_samples_leaf': tune.choice([1, 2, 4])\n",
    "}\n",
    "\n",
    "# SVM-only specific params (active when clf='svm')\n",
    "svm_space = {\n",
    "    'C': tune.loguniform(1e-2, 1e2),\n",
    "    'kernel': tune.choice(['rbf','linear']),\n",
    "    'gamma': tune.choice(['scale','auto'])\n",
    "}\n",
    "\n",
    "def merged_space():\n",
    "    # Conditional sampling inside the trainable is common; for display, we merge dicts\n",
    "    base = dict(search_space)\n",
    "    # Put superset; the objective will read the relevant keys only\n",
    "    base.update({**rf_space, **svm_space})\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ray_tune(X, y, num_samples=20, max_trials=20):\n",
    "    if not HAS_RAY:\n",
    "        print('Ray not available. Please install ray[tune].')\n",
    "        return None\n",
    "    ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
    "    scheduler = ASHAScheduler(metric='roc_auc', mode='max', grace_period=1, reduction_factor=2)\n",
    "    algo = HyperOptSearch(metric='roc_auc', mode='max')\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(tune_objective, X=X, y=y, n_splits=3),\n",
    "        config=merged_space(),\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        search_alg=algo,\n",
    "        resources_per_trial={'cpu': 1},\n",
    "        local_dir='./ray_results',\n",
    "        name='hpo_flexible_model'\n",
    "    )\n",
    "    print('Best config:', analysis.best_config)\n",
    "    print('Best roc_auc:', analysis.best_result['roc_auc'])\n",
    "    return analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda85f6d",
   "metadata": {},
   "source": [
    "## B) tune-sklearn (sklearn-compatible)\n",
    "If installed, you can keep an sklearn-style flow using `TuneSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7919ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tune_sklearn(X, y):\n",
    "    if not HAS_TUNE_SKLEARN:\n",
    "        print('tune-sklearn not available. Please install tune-sklearn.')\n",
    "        return None\n",
    "    # Build a Pipeline estimator\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    est = Pipeline([\n",
    "        ('sc', StandardScaler()),\n",
    "        ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "    ])\n",
    "    # Search space uses ray.tune distributions where possible\n",
    "    param_distributions = {\n",
    "        'clf__n_estimators': tune.qrandint(100, 500, 50),\n",
    "        'clf__max_depth': tune.choice([None, 5, 10, 20]),\n",
    "        'clf__min_samples_leaf': tune.choice([1, 2, 4])\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    tuner = TuneSearchCV(\n",
    "        est, param_distributions=param_distributions,\n",
    "        n_trials=20,\n",
    "        scoring='roc_auc',\n",
    "        cv=cv,\n",
    "        search_optimization='hyperopt',\n",
    "        early_stopping=True,\n",
    "        max_iters=10,\n",
    "        verbose=1\n",
    "    )\n",
    "    tuner.fit(X, y)\n",
    "    print('Best params:', tuner.best_params_)\n",
    "    print('Best score:', tuner.best_score_)\n",
    "    return tuner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea68ff3",
   "metadata": {},
   "source": [
    "## Finalize best model\n",
    "After getting `best_config` from Ray Tune (native), rebuild a final model and `fit` on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_best_model(best_config: Dict[str, Any]):\n",
    "    return make_model_from_config(best_config)\n",
    "\n",
    "# Example:\n",
    "# analysis = run_ray_tune(X_train, y_train, num_samples=30)\n",
    "# best = analysis.best_config\n",
    "# final_model = build_best_model(best)\n",
    "# final_model.fit(X_train, y_train)\n",
    "# y_pred = final_model.predict(X_test)\n",
    "# y_prob = final_model.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whatâ€™s inside:\n",
    "\n",
    "# Native Ray Tune trainable that builds your FlexibleModel from a config, runs Stratified K-Fold, and reports ROC-AUC (with ASHA scheduler + HyperOpt search).\n",
    "# Optional tune-sklearn (TuneSearchCV) example for a scikit-learn-style flow.\n",
    "# Search spaces for RF and SVM with conditional params, plus toggles for scaler and SMOTE.\n",
    "# Helper to rebuild the best model from best_config and fit on your full train set.\n",
    "# Usage (native Ray Tune):\n",
    "\n",
    "# # Assume X_train, y_train are ready\n",
    "# # Optional: !pip install \"ray[tune]\" tune-sklearn imbalanced-learn\n",
    "\n",
    "# from 04_hpo_ray import run_ray_tune, build_best_model  # or run the cells directly\n",
    "# analysis = run_ray_tune(X_train, y_train, num_samples=30)\n",
    "# best = analysis.best_config\n",
    "# final_model = build_best_model(best)\n",
    "# final_model.fit(X_train, y_train)\n",
    "# y_pred  = final_model.predict(X_test)\n",
    "# y_prob  = final_model.predict_proba(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
