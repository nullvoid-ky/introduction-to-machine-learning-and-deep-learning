{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, X, y, metrics=None, average='binary', verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate any trained model with common metrics and optional plots.\n",
    "    Works for both FlexibleModel and regular sklearn models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Trained model with predict() and optionally predict_proba().\n",
    "    X : pd.DataFrame or np.ndarray\n",
    "        Test features.\n",
    "    y : pd.Series or np.ndarray\n",
    "        True labels.\n",
    "    metrics : list[str], optional\n",
    "        Which metrics to compute (default: all).\n",
    "        Options: ['accuracy','precision','recall','f1','roc_auc']\n",
    "    average : str, default='binary'\n",
    "        Averaging method for F1/Precision/Recall when y has >2 classes.\n",
    "    verbose : bool, default=True\n",
    "        Whether to print and plot results.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Metric results and confusion matrix DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "    y_pred = model.predict(X, y)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X)\n",
    "        y_prob = np.asarray(y_prob).reshape(-1)\n",
    "    except Exception:\n",
    "        y_prob = None\n",
    "\n",
    "    results = {}\n",
    "    if 'accuracy' in metrics:\n",
    "        results['accuracy'] = accuracy_score(y, y_pred)\n",
    "    if 'precision' in metrics:\n",
    "        results['precision'] = precision_score(y, y_pred, average=average, zero_division=0)\n",
    "    if 'recall' in metrics:\n",
    "        results['recall'] = recall_score(y, y_pred, average=average, zero_division=0)\n",
    "    if 'f1' in metrics:\n",
    "        results['f1'] = f1_score(y, y_pred, average=average, zero_division=0)\n",
    "    if 'roc_auc' in metrics and y_prob is not None and len(np.unique(y)) == 2:\n",
    "        results['roc_auc'] = roc_auc_score(y, y_prob)\n",
    "\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                         index=[f'True_{c}' for c in np.unique(y)],\n",
    "                         columns=[f'Pred_{c}' for c in np.unique(y)])\n",
    "    results['confusion_matrix'] = cm_df\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ðŸ“Š Model Evaluation Summary\")\n",
    "        print(pd.Series(results).drop('confusion_matrix', errors='ignore'))\n",
    "        print(\"\\nConfusion Matrix:\\n\", cm_df)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y, y_pred, zero_division=0))\n",
    "\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fceb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you already have: model.fit(X_train, y_train)\n",
    "# eval_results = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# # Access metrics directly\n",
    "# print(eval_results['accuracy'])\n",
    "# print(eval_results['roc_auc'])\n",
    "# eval_results['confusion_matrix']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
