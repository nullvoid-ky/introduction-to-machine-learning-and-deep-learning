{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa174dfd",
   "metadata": {},
   "source": [
    "# ðŸ§  AI / Machine Learning / Deep Learning â€” Ultimate Cheat Sheet\n",
    "*Generated: 2025-10-14 18:05*\n",
    "\n",
    "---\n",
    "**How to use:**\n",
    "- This notebook is a living cheat sheet: scan, copy, adapt.\n",
    "- Each section has minimal runnable code + links to deeper blocks.\n",
    "- Use `CTRL/CMD + F` to jump, or JupyterLab ToC for navigation.\n",
    "\n",
    "> Tip: Keep your dataset-specific cells at the bottom in the Playground section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430b614",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Boilerplate & Reproducibility](#boilerplate)\n",
    "2. [Math Quickies (Linear Algebra & Prob/Stats)](#math)\n",
    "3. [Data Loading & Cleaning](#data)\n",
    "4. [EDA & Visualization](#eda)\n",
    "5. [Feature Engineering & Selection](#fe)\n",
    "6. [Modeling Patterns (sklearn)](#sklearn)\n",
    "7. [Imbalanced Learning (SMOTE, Class Weights, Bagging)](#imbalance)\n",
    "8. [Evaluation Metrics & Curves (ROC/PR)](#metrics)\n",
    "9. [Cross-Validation & Pipelines](#cv)\n",
    "10. [From-Scratch Mini-Implementations](#scratch)\n",
    "11. [PyTorch Templates](#torch)\n",
    "12. [TensorFlow/Keras Templates](#keras)\n",
    "13. [Hyperparameter Tuning (Grid/Random/Optuna-lite)](#tuning)\n",
    "14. [Model Interpretability (SHAP/LIME tips)](#interpret)\n",
    "15. [Saving, Loading, and Deployment Snips](#deploy)\n",
    "16. [Troubleshooting & Performance Tips](#troubleshoot)\n",
    "17. [Playground / Your Dataset Hooks](#playground)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3f987",
   "metadata": {},
   "source": [
    "## 1) Boilerplate & Reproducibility <a id='boilerplate'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, gc, json, random, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # use default styles (no seaborn)\n",
    "from pprint import pprint\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "def seed_everything(seed=RANDOM_STATE):\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "seed_everything()\n",
    "\n",
    "def tic():\n",
    "    return time.time()\n",
    "\n",
    "def toc(t0, label='Elapsed'):\n",
    "    print(f\"{label}: {time.time()-t0:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179b766",
   "metadata": {},
   "source": [
    "## 2) Math Quickies (Linear Algebra & Prob/Stats) <a id='math'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector norms & cosine similarity\n",
    "a = np.array([1,2,3]); b = np.array([4,5,6])\n",
    "l2_a = np.linalg.norm(a)\n",
    "cos = a @ b / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "print('||a||2 =', l2_a)\n",
    "print('cos(a,b)=', cos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid, Softmax, Cross-Entropy\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=-1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum(axis=-1, keepdims=True)\n",
    "def binary_cross_entropy(y, p, eps=1e-9):\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return -(y*np.log(p) + (1-y)*np.log(1-p)).mean()\n",
    "print('sigmoid(0)=', sigmoid(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50e2b1",
   "metadata": {},
   "source": [
    "## 3) Data Loading & Cleaning <a id='data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV quickly\n",
    "CSV_PATH = 'your_data.csv'  # change me\n",
    "if Path(CSV_PATH).exists():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    display(df.head())\n",
    "else:\n",
    "    print('> Tip: set CSV_PATH to your file path and re-run.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning template\n",
    "def quick_clean(df):\n",
    "    df = df.copy()\n",
    "    # Strip whitespace col names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    # Drop dupes\n",
    "    df = df.drop_duplicates()\n",
    "    # Example: fill numeric NA with median\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for c in num_cols:\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "    return df\n",
    "\n",
    "try:\n",
    "    df = quick_clean(df)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a3841",
   "metadata": {},
   "source": [
    "## 4) EDA & Visualization <a id='eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist & correlation (matplotlib only)\n",
    "def plot_hist(df, col):\n",
    "    plt.figure()\n",
    "    df[col].plot(kind='hist', bins=30)\n",
    "    plt.title(f'Histogram: {col}')\n",
    "    plt.show()\n",
    "\n",
    "def quick_corr(df):\n",
    "    plt.figure()\n",
    "    plt.imshow(df.corr(numeric_only=True), aspect='auto')\n",
    "    plt.title('Correlation (numeric only)')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da83f6",
   "metadata": {},
   "source": [
    "## 5) Feature Engineering & Selection <a id='fe'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad766deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def basic_numeric_preprocess(df, features):\n",
    "    num_pipe = Pipeline([\n",
    "        ('impute', SimpleImputer(strategy='median')),\n",
    "        ('scale', StandardScaler()),\n",
    "    ])\n",
    "    pre = ColumnTransformer([\n",
    "        ('num', num_pipe, features)\n",
    "    ])\n",
    "    return pre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72f81e",
   "metadata": {},
   "source": [
    "## 6) Modeling Patterns (sklearn) <a id='sklearn'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "def fit_basic_models(X, y, features=None, test_size=0.2, rs=RANDOM_STATE):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, stratify=y, random_state=rs)\n",
    "    if features is None:\n",
    "        features = X.columns.tolist()\n",
    "    pre = basic_numeric_preprocess(X, features)\n",
    "    models = {\n",
    "        'LR': LogisticRegression(max_iter=1000, n_jobs=None),\n",
    "        'DT': DecisionTreeClassifier(random_state=rs),\n",
    "        'RF': RandomForestClassifier(n_estimators=200, random_state=rs, n_jobs=-1),\n",
    "        'GB': GradientBoostingClassifier(random_state=rs)\n",
    "    }\n",
    "    results = {}\n",
    "    for name, clf in models.items():\n",
    "        pipe = Pipeline([('pre', pre), ('clf', clf)])\n",
    "        t0 = tic(); pipe.fit(Xtr, ytr); toc(t0, f'Fit {name}')\n",
    "        if hasattr(pipe.named_steps['clf'], 'predict_proba'):\n",
    "            yprob = pipe.predict_proba(Xte)[:,1]\n",
    "        else:\n",
    "            # Fallback: decision_function â†’ sigmoid-like mapping (rough)\n",
    "            score = pipe.decision_function(Xte)\n",
    "            yprob = (score - score.min())/(score.max()-score.min()+1e-9)\n",
    "        ypred = pipe.predict(Xte)\n",
    "        results[name] = {\n",
    "            'acc': accuracy_score(yte, ypred),\n",
    "            'f1': f1_score(yte, ypred),\n",
    "            'roc_auc': roc_auc_score(yte, yprob)\n",
    "        }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce9efb",
   "metadata": {},
   "source": [
    "## 7) Imbalanced Learning (SMOTE, Class Weights, Bagging) <a id='imbalance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "def smote_rf_template(X, y, features=None, test_size=0.2):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, stratify=y, random_state=RANDOM_STATE)\n",
    "    if features is None:\n",
    "        features = X.columns.tolist()\n",
    "    pre = basic_numeric_preprocess(X, features)\n",
    "    pipe = ImbPipeline([\n",
    "        ('pre', pre),\n",
    "        ('smote', SMOTE(random_state=RANDOM_STATE, k_neighbors=5, sampling_strategy='auto')),\n",
    "        ('clf', RandomForestClassifier(n_estimators=300, class_weight='balanced_subsample', random_state=RANDOM_STATE, n_jobs=-1))\n",
    "    ])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    yprob = pipe.predict_proba(Xte)[:,1]\n",
    "    ypred = pipe.predict(Xte)\n",
    "    print('F1:', f1_score(yte, ypred), ' ROC-AUC:', roc_auc_score(yte, yprob))\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BalancedBagging (no augmentation) template\n",
    "def balanced_bagging_rf(X, y, features=None, test_size=0.2):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, stratify=y, random_state=RANDOM_STATE)\n",
    "    if features is None:\n",
    "        features = X.columns.tolist()\n",
    "    pre = basic_numeric_preprocess(X, features)\n",
    "    base_rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    clf = BalancedBaggingClassifier(base_estimator=base_rf, n_estimators=10, sampling_strategy='auto', random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    pipe = ImbPipeline([('pre', pre), ('clf', clf)])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    yprob = pipe.predict_proba(Xte)[:,1]\n",
    "    ypred = pipe.predict(Xte)\n",
    "    print('F1:', f1_score(yte, ypred), ' ROC-AUC:', roc_auc_score(yte, yprob))\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790fedff",
   "metadata": {},
   "source": [
    "## 8) Evaluation Metrics & Curves (ROC/PR) <a id='metrics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "def plot_roc_pr(y_true, y_prob):\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC AUC={roc_auc:.3f}')\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve'); plt.legend(); plt.show()\n",
    "    # PR\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'PR AUC={pr_auc:.3f}')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curve'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf81651",
   "metadata": {},
   "source": [
    "## 9) Cross-Validation & Pipelines <a id='cv'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cv_auc(pipe_builder, X, y, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    for fold, (tr, te) in enumerate(tqdm(skf.split(X, y), total=n_splits, desc='CV folds')):\n",
    "        Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "        ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "        pipe = pipe_builder()\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        yprob = pipe.predict_proba(Xte)[:,1]\n",
    "        scores.append(roc_auc_score(yte, yprob))\n",
    "    print('ROC-AUC meanÂ±std:', np.mean(scores), np.std(scores))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc313c9",
   "metadata": {},
   "source": [
    "## 10) From-Scratch Mini-Implementations <a id='scratch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ab54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression (binary) â€” minimal from-scratch\n",
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, lr=0.1, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.w = None\n",
    "        self.b = 0.0\n",
    "    def fit(self, X, y):\n",
    "        X = X.astype(float)\n",
    "        y = y.astype(float)\n",
    "        n, d = X.shape\n",
    "        self.w = np.zeros(d)\n",
    "        for _ in range(self.n_iter):\n",
    "            z = X @ self.w + self.b\n",
    "            p = 1/(1+np.exp(-z))\n",
    "            grad_w = X.T @ (p - y) / n\n",
    "            grad_b = (p - y).mean()\n",
    "            self.w -= self.lr * grad_w\n",
    "            self.b -= self.lr * grad_b\n",
    "    def predict_proba(self, X):\n",
    "        z = X @ self.w + self.b\n",
    "        return 1/(1+np.exp(-z))\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini impurity and a toy split function (for DT intuition)\n",
    "def gini(labels):\n",
    "    if len(labels)==0: return 0.0\n",
    "    p = np.mean(labels)\n",
    "    return 2*p*(1-p)\n",
    "print('gini([0,0,1,1,1])=', gini(np.array([0,0,1,1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17066163",
   "metadata": {},
   "source": [
    "## 11) PyTorch Templates <a id='torch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, in_dim, hidden=64, out_dim=1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "                nn.Linear(hidden, out_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    print('PyTorch ready âœ”')\n",
    "except Exception as e:\n",
    "    print('PyTorch not available â†’', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b207482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal train loop (binary classification w/ BCEWithLogitsLoss)\n",
    "def torch_train_loop(model, loader, epochs=5, lr=1e-3, device='cpu'):\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    model.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        total = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).float().view(-1,1)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward(); opt.step()\n",
    "            total += loss.item()*len(xb)\n",
    "        print(f'Epoch {ep}: loss={total/len(loader.dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce79782",
   "metadata": {},
   "source": [
    "## 12) TensorFlow/Keras Templates <a id='keras'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    def build_keras_mlp(in_dim):\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(in_dim,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC','Precision','Recall'])\n",
    "        return model\n",
    "    print('TensorFlow ready âœ”')\n",
    "except Exception as e:\n",
    "    print('TensorFlow not available â†’', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44580b1",
   "metadata": {},
   "source": [
    "## 13) Hyperparameter Tuning (Grid/Random) <a id='tuning'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "def grid_search_example(X, y, features=None):\n",
    "    if features is None: features = X.columns.tolist()\n",
    "    pre = basic_numeric_preprocess(X, features)\n",
    "    pipe = Pipeline([('pre', pre), ('clf', RandomForestClassifier(random_state=RANDOM_STATE))])\n",
    "    grid = {\n",
    "        'clf__n_estimators': [100, 300],\n",
    "        'clf__max_depth': [None, 10, 20],\n",
    "    }\n",
    "    gs = GridSearchCV(pipe, grid, scoring='roc_auc', cv=3, n_jobs=-1, verbose=1)\n",
    "    gs.fit(X, y)\n",
    "    print('Best:', gs.best_params_, ' Score:', gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383982e",
   "metadata": {},
   "source": [
    "## 14) Model Interpretability (SHAP/LIME tips) <a id='interpret'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c100b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tips: For tree models use TreeExplainer; subsample to speed up. For linear models, coefficients are direct indicators. For NN, use Integrated Gradients or Deep SHAP when available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f25021",
   "metadata": {},
   "source": [
    "## 15) Saving, Loading, and Deployment Snips <a id='deploy'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe21e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "def save_model(obj, path='model.joblib'):\n",
    "    joblib.dump(obj, path)\n",
    "    print('Saved â†’', path)\n",
    "def load_model(path='model.joblib'):\n",
    "    return joblib.load(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da049faf",
   "metadata": {},
   "source": [
    "## 16) Troubleshooting & Performance Tips <a id='troubleshoot'></a>\n",
    "- **Long training?** Reduce estimators, features, or sample rows; profile bottlenecks.\n",
    "- **Imbalanced metrics misleading?** Use ROC-AUC/PR-AUC, check confusion matrix per threshold.\n",
    "- **Smote k_neighbors:** try 3â€“10; validate via CV; avoid leakage by fitting on train only.\n",
    "- **Reproducibility:** fix seeds and library versions.\n",
    "- **Memory:** delete big objects (`del var` + `gc.collect()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893964a4",
   "metadata": {},
   "source": [
    "## 17) Playground / Your Dataset Hooks <a id='playground'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example wiring: set FEATURES and TARGET then run a pipeline\n",
    "try:\n",
    "    FEATURES = [c for c in df.columns if c != 'status_label']  # change as needed\n",
    "    TARGET = 'status_label'  # change as needed\n",
    "    X = df[FEATURES].copy(); y = df[TARGET].copy()\n",
    "    print('Fitting basic models...')\n",
    "    res = fit_basic_models(X, y, features=FEATURES)\n",
    "    pprint(res)\n",
    "except NameError:\n",
    "    print('> Load your dataframe as df first (see Section 3).')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
